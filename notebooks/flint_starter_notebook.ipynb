{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Add the \"bootstrap\" facility to the notebook\n",
    "interp.load.ivy(\"com.videoamp\" %% \"ammonium-util\" % \"2.0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Import the ammonite.ops package members. See http://ammonite.io/#Ammonite-Ops for more info\n",
    "import ammonite.ops._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Set this to the IP address of your Spark master\n",
    "val masterIP = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Download the Spark libraries from the master and prepare Spark for configuration\n",
    "// Execute this once the master node is available\n",
    "vamp.ammonium.bootstrap(masterIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// The Scala kernel can run so-called Scala scripts. See http://ammonite.io/#ScalaScripts\n",
    "// for more information. For example, to load the PostgreSQL JDBC driver and configure\n",
    "// it for use with Spark, run\n",
    "// interp.load.module(root/'home/'jupyter/'scripts/\"postgresql.sc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// This script defines an implicit class which adds the display() and display(count: Int) methods\n",
    "// to the DataFrame type. Consider these methods to be more readable alternatives to show() and\n",
    "// show(count: Int).\n",
    "//\n",
    "// interp.load.module(root/'home/'jupyter/'scripts/\"displayable_data_frame.sc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Spark Configuration\n",
    "\n",
    "// Set your Spark app's name\n",
    "val appName = \"\"\n",
    "\n",
    "// Set your app's core count. The default value configures Spark to allocate all available cores\n",
    "// to your app\n",
    "val appCores = Int.MaxValue\n",
    "\n",
    "// Set the default level of parallelism for your Spark app. The recommended minimum is 512,\n",
    "// and can be increased by factors of 2 to help address executor OOM errors in apps with\n",
    "// large storage memory requirements\n",
    "val parallelism = 512\n",
    "\n",
    "// Set your desired executor heap size (in GB). The recommended range is 8 to 16. The JVM can\n",
    "// struggle collecting garbage efficiently in Spark executors with large heap spaces\n",
    "val executorHeapGB = 16\n",
    "\n",
    "// The number of cluster cores to assign to each executor. To run one executor per worker, set\n",
    "// this to the number of cores per worker node. To run two executors per worker, set it to half\n",
    "// the number of cores per worker, etc. One executor per worker is recommended unless you need\n",
    "// more than 16 GB of heap per worker. In this case, two executors with 16 GB of heap each is\n",
    "// recommended rather than one executor with 32 GB\n",
    "val coresPerExecutor = 32\n",
    "\n",
    "// Set this to your worker's maximum allocated memory (in GB). 50 is recommended for c3.8xl\n",
    "// workers and 220 is recommended for r3.8xl workers\n",
    "val totalExecutorMemoryGB = 50\n",
    "\n",
    "// This allocates the remainder of your worker's memory to off-heap memory. Do not change this\n",
    "// unless you have good reason\n",
    "val executorOffHeapBytes = (totalExecutorMemoryGB - executorHeapGB) * (1024 * 1024 * 1024).toLong\n",
    "\n",
    "// These are recommended executor JVM flags\n",
    "val executorFlags =\n",
    "  \"-XX:+UseParallelGC\" ::\n",
    "  \"-XX:+HeapDumpOnOutOfMemoryError\" ::\n",
    "  \"-XX:HeapDumpPath=/scratch1/heapdumps\" ::\n",
    "  \"-XX:+PrintClassHistogram\" ::\n",
    "  Nil\n",
    "\n",
    "// These tune some advanced settings to recommended values\n",
    "sparkConf\n",
    "  .set(\"spark.driver.maxResultSize\", \"2048\")\n",
    "  .set(\"spark.kryoserializer.buffer.max\", \"1g\")\n",
    "  .set(\"spark.rdd.compress\", \"true\")\n",
    "\n",
    "// You can set Hadoop configuration properties by prefixing a Hadoop configuration key with\n",
    "// \"spark.hadoop\". For example, to set the default HDFS replication level to 2:\n",
    "// sparkConf\n",
    "//   .set(\"spark.hadoop.dfs.replication\", \"2\")\n",
    "\n",
    "// Uncomment to set additional configuration here\n",
    "// sparkConf\n",
    "//   .set(\"spark.foo\", \"bar\")\n",
    "//   .set(\"spark.biz\", \"baz\")\n",
    "\n",
    "// These settings simply reflect the values set above. Do not modify this\n",
    "sparkConf\n",
    "  .setAppName(appName)\n",
    "  .set(\"spark.cores.max\", appCores.toString)\n",
    "  .set(\"spark.executor.cores\", coresPerExecutor.toString)\n",
    "  .set(\"spark.default.parallelism\", parallelism.toString)\n",
    "  .set(\"spark.sql.shuffle.partitions\", parallelism.toString)\n",
    "  .set(\"spark.executor.memory\", executorHeapGB + \"g\")\n",
    "  .set(\"spark.memory.offHeap.enabled\", \"true\")\n",
    "  .set(\"spark.memory.offHeap.size\", executorOffHeapBytes.toString)\n",
    "  .set(\"spark.executor.extraJavaOptions\", executorFlags.mkString(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// At this point you should have access to a `SparkSession` from the `spark` val, e.g.\n",
    "// spark.table(\"mydumbdatabase.mydumbtable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "// Helpful imports\n",
    "\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.storage.StorageLevel.OFF_HEAP\n",
    "\n",
    "import org.apache.spark.sql.{ Column, DataFrame, Dataset }\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "import spark.implicits._\n",
    "import spark.table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.11",
   "language": "scala",
   "name": "scala211"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "script",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
